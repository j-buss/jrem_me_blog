<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on jrem.me - wonder.explore.share</title>
    <link>/tags/r/</link>
    <description>Recent content in R on jrem.me - wonder.explore.share</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Jan 2022 00:00:00 +0000</lastBuildDate><atom:link href="/tags/r/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Linear Regression</title>
      <link>/2022/01/03/linear-regression/</link>
      <pubDate>Mon, 03 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/2022/01/03/linear-regression/</guid>
      <description>Interaction: two or more variables interact to affect a third variable in a non additive manner, i.e. two variables interact to have an affect more than the sum of their parts Simple Linear Regression (formula): \(Y = \beta_0 + \beta_1 X + \epsilon\) 2 Coefficients or parameters for Simple Linear Regression (SLR): \(\beta_0=\) Intercept \(\beta_1=\) slope SLR estimate (formula): \(\hat{y_i} = \hat{\beta}_0 + \hat{\beta}_1 x_i\) Residual: difference between the \(i\)th observed response and the \(i\)th response value predicted by our model Residual (formula): \(e_i = y_i - \hat{y}_i\) Residual Sum of Squares: the sum of squared residuals for all observations \(i=1, 2, \dots, n\) a.</description>
    </item>
    
    <item>
      <title>Introduction and Linear Algebra Review</title>
      <link>/2022/01/01/introduction-and-linear-algebra-review/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/2022/01/01/introduction-and-linear-algebra-review/</guid>
      <description>Notation and Simple Matrix Algebra n: number of observations
\(x_{ij}\): the \(j\)-th variable of the \(i\)th observation where \(i=1,2,\dots,n\) and \(j=1,2,\dots,p\)
p: number of variables
\(\mathbf{X}\): is a matrix of size \(n \times p\) whose \((i,j)\)th element is \(x_{ij}\)
\(\mathbf{X}=\) \(\begin{pmatrix} x_{11}&amp;amp;x_{12}&amp;amp;\dots&amp;amp;x_{1p} \\x_{21}&amp;amp;x_{22}&amp;amp;\dots&amp;amp;x_{2p}\\\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots\\x_{n1}&amp;amp;x_{n2}&amp;amp;\dots&amp;amp;x_{np} \end{pmatrix}\)
Row vector of \(\mathbf{X}\): \(x_i = \begin{pmatrix} x_{i1}\\x_{i2}\\\vdots\\x_{ip} \end{pmatrix}\);
*Note: even though it is a row vector the default orientation is as a column Column vector of \(\mathbf{X}\): \(\mathbf{x}_j = \begin{pmatrix} x_{1j}\\x_{2j}\\\vdots\\x_{nj} \end{pmatrix}\)</description>
    </item>
    
    <item>
      <title>Statistical Learning</title>
      <link>/2022/01/01/statistical-learning/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/2022/01/01/statistical-learning/</guid>
      <description>Input Variable (X): predictor, independent variable, feature Output Variable(Y): response, dependent variable error term (\(\epsilon\)): assumed independent of \(X, Y\) w/ mean of zero General form of relationship between \(X\)s and \(Y\): \(Y=f(X) + \epsilon\) Estimated relationship: We don’t know \(f(X)\) so we estimate it by using the data to fit a relationship \(\hat{Y}=\hat{f}(X)\) Two reasons to estimate an unknown function \(\hat{f}\) relating input vs. output: Prediction and Inference Error is composed of 2 categories: Reducible and Irreducible Reducible Error: Potentially improve accuracy of \(\hat{f}\) by using the most appropriate statistical learning techniques Irreducible Error: may contain unmeasured items that can’t be used to understand or predict \(Y\) 2 Traits of Parametric Model: 1 - Makes an assumption about the form of the model 2 - Use data to estimate relatively few parameters of the assumed form Non-Parametric Model - No assumptions about the form of the model are made Overfitting: Model follows errors or noise too closely Advantage of Parametric Model: Assumed model requires few estimated parameters Disadvantage of Parametric Model: Assumed model form may not follow true form Advantage of Non-Parametric Model: No assumed form Disadvantage of Non-Parametric Model: Lots of data required to fit model Why would we ever choose to uyse a more restrictive model instead of a very flexible approach?</description>
    </item>
    
    <item>
      <title>Model Adequacy Checking - Examples from Montgomery/Peck/Vining</title>
      <link>/2021/10/23/model-adequacy-checking-examples-from-montgomery/peck/vining/</link>
      <pubDate>Sat, 23 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/10/23/model-adequacy-checking-examples-from-montgomery/peck/vining/</guid>
      <description>For the example data we will leverage the CRAN package “MPV”.
#install.packages(&amp;quot;MPV&amp;quot;) library(MPV) Example 4.1: The Delivery Time Data Let’s define the dataframe for the SoftDrink data and take a look at the first 5 rows:
SoftDrink &amp;lt;- data.frame(MPV::softdrink) colnames(SoftDrink) &amp;lt;- c(&amp;quot;DeliveryTime&amp;quot;, &amp;quot;NumberCases&amp;quot;, &amp;quot;Distance&amp;quot;) head(SoftDrink, 5) ## DeliveryTime NumberCases Distance ## 1 16.68 7 560 ## 2 11.50 3 220 ## 3 12.03 3 340 ## 4 14.88 4 80 ## 5 13.</description>
    </item>
    
    <item>
      <title>Model Adequacy Checking</title>
      <link>/2021/10/19/model-adequacy-checking/</link>
      <pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/10/19/model-adequacy-checking/</guid>
      <description>Assumptions of Linear Regression Linear Regression:
\[ \begin{align*} Y_i = \beta_0 + \beta_1 X_{i1} + \dots + \beta_k X_{ik} + \epsilon_i \end{align*} \]
The major assumptions that we have made concerning Linear Regression are as follows:
Relationship between \(Y\) and \(X_i\) is linear \(E(\epsilon_i)=0\) \(Var(\epsilon_i)=\sigma^2\) \(Cov(\epsilon_i, \epsilon_j)=0\) for all \(i \ne j\) \(\epsilon_i {\overset {iid}{\sim}} N(0,\sigma^2)\) If assumptions are violated, a different sample could lead to different model with opposite conclusions!</description>
    </item>
    
  </channel>
</rss>
