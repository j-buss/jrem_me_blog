---
title: "Chapter 6 - Special Probability Densities"
author: Jeremy Buss
output: html_document
date: "`r Sys.Date()`"
categories:
  - John E. Freund's Mathematical Statistics with Applications - Miller/Miller
  - Statistics
draft: yes
katex: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r message=FALSE, warning=FALSE, include=FALSE}
install.packages("blogdown")
```



## 6.1 Introduction

## 6.2 The Uniform Distribution

**Uniform Distribution** A random variable \(X\) has a **uniform distribution** and it is referred to as a continuous uniform random varaible if and only if its probability density is given by

\[
u(x;\alpha,\beta)= 
\begin{cases}
\frac{1}{\beta - \alpha} & \text{for } \alpha \lt x \lt \beta \\
0 & \text{elsewhere}
\end{cases}
\]

**Theorem 6.1 [Uniform Distribution mean and variance]** The mean and variance of the uniform distribution are given by

\[
\mu = \frac{\alpha + \beta}{2} \;\; \text{and} \;\; \sigma^2=\frac{1}{12}(\beta - \alpha)^2
\]

## 6.3 The Gamma, Exponential, and Chi-Square Distributions

**Gamma Function Integral Definition** 

\[
\Gamma(\alpha) = \int_0^{\infty} y^{\alpha-1} e^{-y} dy \; \; \text{for } \alpha \gt 0
\]

**Gamma Function Factorial Definition** 

\[
\Gamma(\alpha) = (\alpha - 1) \Gamma(\alpha - 1)
\]

or another way

\[
\Gamma(\alpha) = (\alpha - 1)!
\]

**Gamma Distribution** - A random variable \(X\) has a **gamma distribution** and it is referred to as a gamma random variable if and only if its probability density is given by

\[
g(x; \alpha, \beta)
\begin{cases}
\frac{1}{\beta^\alpha \Gamma(\alpha)}x^{\alpha - 1} e^{-x/\beta} & \text{for } x \gt 0\\
0 & \text{elsewhere}
\end{cases}
\]

where \(\alpha > 0\) and \(\beta \gt 0\)

**Exponential Distribution** - A random variable \(X\) has an **exponential distribution** and it is referred to as an exponential random variable if and only if its probability density is given by

\[
g(x; \theta)
\begin{cases}
\frac{1}{\theta} e^{-x/\theta} & \text{for } x \gt 0\\
0 & \text{elsewhere}
\end{cases}
\]

where \(\theta > 0\)

**Chi-Square Distribution** - A random variable \(X\) has a **chi-square distribution** and it is referred to as a chi-square random variable if and only if its probability density is given by

\[
f(x; \nu)
\begin{cases}
\frac{1}{2^{\nu/2} \Gamma(\nu / 2)} x^{\frac{\nu-2}{2}} e^{\frac{-x}{2}} & \text{for } x \gt 0\\
0 & \text{elsewhere}
\end{cases}
\]

where \(\theta > 0\)

**Theorem 6.2 [Gamma Distribution - \(r\)th moment about Origin]**

\[
\mu^{'}_r = \frac{\beta^r\Gamma(\alpha + r)}{\Gamma(\alpha)}
\]

**Theorem 6.2 [Gamma Distribution - \(r\)th moment about Origin - Proof]**

Using definition of moments about the origin

\[
\mu^{'}_r = \int_o^{\infty} x^r \cdot \frac{1}{\beta^{\alpha} \Gamma(\alpha)} x^{\alpha-1} e^{-x/\beta} dx = \frac{\beta^r}{\Gamma(\alpha)} \cdot \int_0^{\infty} y^{\alpha+r-1} e^{-y} dy
\]

where we let \(y=\frac{x}{\beta}\). Since the integral on the right is \(\Gamma(r+\alpha)\) according to the definition of the gamma function.

**Theorem 6.3 [Gamma distribution mean and variance]**

\[
\mu = \alpha \beta \;\; \text{and} \;\; \sigma^2 = \alpha \beta^2
\]

**Theorem 6.3 [Gamma distribution mean and variance - Proof]**

Using Theorem 6.2 [Gamma Distribution - \(r\)th moment about Origin] with \(r=1\) and \(r=2\), we get

\[
\mu_1^{'} = \frac{\beta \Gamma (\alpha + 1)}{\Gamma(\alpha)} = \alpha \beta
\]

and 

\[
\mu_2^{'} = \frac{\beta^2 \Gamma (\alpha + 2)}{\Gamma(\alpha)} = \alpha (\alpha + 1) \beta^2
\]

so \(\mu=\alpha \beta \) and \(\sigma^2=\alpha (\alpha + 1 )\beta^2 - (\alpha \beta)^2=\alpha \beta^2\)

**Exponential Distribution [mean and variance]**

\[
\mu = \theta \;\; \text{and} \;\; \sigma^2=\theta^2
\]

**Chi-square Distribution [mean and variance]**

\[
\mu = \nu \;\; \text{and} \;\; \sigma^2=2\nu
\]

**Theorem 6.4 [Gamma Distribution Moment Generating Function]**

\[
M_X(t)=(1-\beta t)^{-\alpha}
\]

## 6.4 The Beta Distribution

**Beta Distribution** - A random variable \(X\) has an **beta distribution** and it is referred to as a beta random variable if and only if its probability density is given by

\[
f(x; \alpha, \beta)
\begin{cases}
\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \cdot \Gamma(\beta)} x^{\alpha -1} (1-x)^{\beta -1} & \text{for } 0 \lt x \lt 1\\
0 & \text{elsewhere}
\end{cases}
\]

where \(\alpha \gt 0\) and \(\beta \gt 0\)

**Beta Distribution [mean and variance]** 

\[
\mu = \frac{\alpha}{\alpha + \beta} \;\; \text{and} \;\; \sigma^2 = \frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}
\]

**Beta Distribution [mean and variance - Proof]** 

By definition

\[
\begin{align}
&=\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \cdot \Gamma(\beta)} \cdot \int_0^1 x \cdot x^{\alpha - 1} (1-x)^{\beta-1} dx\\
&=\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \cdot \Gamma(\beta)} \cdot \frac{\Gamma(\alpha + 1) \cdot \Gamma(\beta)}{\Gamma(\alpha + \beta + 1)}\\
&=\frac{\alpha}{\alpha + \beta}
\end{align}
\]

where we recognized the integral as \(B(\alpha + 1, \beta)\) and made use of the fact that \(\Gamma(\alpha + 1)=\alpha \cdot \Gamma (\alpha)\) and \(\Gamma(\alpha + \beta + 1)=(\alpha + \beta) \cdot \Gamma (\alpha + \beta)\). Similar steps yield

\[
\mu_2^{'}= \frac{(\alpha + 1)\alpha}{(\alpha + \beta + 1)(\alpha + \beta)}
\]

and if follows that

\[
\begin{align}
\sigma^2&=\frac{(\alpha + 1)\alpha}{(\alpha + \beta + 1)(\alpha + \beta)} - \left ( \frac{\alpha}{\alpha + \beta} \right )^2\\
&=\frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta+1)}
\end{align}
\]

## 6.5 The Normal Distribution

**Normal Distribution** A random variable \(X\) has a **normal distribution** and it is referred to as a normal random variable if and only if its probability density is given by

\[
n(x;\mu,\sigma)=\frac{1}{\sigma \sqrt{2 \pi}}e^{-\frac{1}{2} \left ( \frac{x-\mu}{\sigma} \right )^2} \;\; \text{for} \, -\infty \lt x \lt \infty
\]

where \(\sigma \gt 0 \)

**Normal Distribution [Moment generating function]**

\[
M_X(t)=e^{\mu t + \frac{1}{2}\sigma^2 t^2}
\]

**Normal Distribution [Moment generating function - Proof]**

By definition

\[
\begin{align}
M_X(t)&=\int_{-\infty}^{\infty} e^{xt} \cdot \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2} \left ( \frac{x-\mu}{\sigma} \right ) ^2} \\
&=\frac{1}{\sigma \sqrt{2 \pi}} \cdot \int_{-\infty}^{\infty} e^{-\frac{1}{2 \sigma^2} [-2xt\sigma^2+(x-\mu)^2]} dx 
\end{align}
\]

and if we complete the square, that is use the identity

\[
-2xt\sigma^2 + (x-\mu)^2 = [x-(\mu+t \sigma^2)]^2 - 2\mu t \sigma^2 - t^2 \sigma^4
\]

we get

\[
M_X(t) = e^{\mu t + \frac{1}{2} t^2 \sigma^2} \left \{ \frac{1}{\sigma \sqrt{2 \pi}} \cdot \int_{- \infty}^{\infty} e^{-\frac{1}{2} \left [ \frac{x-(\mu+t\sigma^2)}{\sigma} \right ]^2} dx \right \}
\]

Since the quantity inside the braces is the integral from \(-\infty\) to \(\infty\) of a normal density with the parameters \(\mu + t \sigma^2\) and \(\sigma\), and hence is equal to 1, it follows that 

\[
M_X(t) = e^{\mu t + \frac{1}{2} \sigma^2 t^2}
\]

**Standard Normal Distribution** - The normal distribution with \(\mu=0\) and \(\sigma=1\) is referred to as the **standard normal distribution**

**Theorem 6.7 [Z-score]** If \(X\) has a normal distribution with the mean \(\mu\) and the standard deviation \(\sigma\), then

\[
Z= \frac{X-\mu}{\sigma}
\]

has the standard normal distribution.

**Theorem 6.7 [Z-score - Proof]**

Since the relationship between the values of \(X\) and \(Z\) is linear, \(Z\) must take on a value between \(z_1=\frac{x_1 - \mu}{\sigma}\) and \(z_2=\frac{x_2 - \mu}{\sigma}\) when \(X\) takes on a value between \(x_1\) and \(x_2\). Hence we can write:

\[
\begin{align}
P(x_1 \lt X \lt x_2)&=\frac{1}{\sqrt{2 \pi \sigma}} \int_{x_1}^{x_2} e^{-\frac{1}{2} \left ( \frac{x-\mu}{\sigma} \right )^2} dx\\
&=\frac{1}{\sqrt{2 \pi}} \int_{z_1}^{z_2} e^{-\frac{1}{2} z^2} dz\\
&=\int_{z_1}^{z_2} n(z;0,1) dz\\
&=P(z_1 \lt Z \lt z_2)\\
\end{align}
\]

Where \(Z\) is seen to be a random variable having the standard normal distribution

## 6.6 The Normal Approximation to the Binomial Distribution

**Theorem 6.8** 

## 6.7 Bivariate Normal Distribution

