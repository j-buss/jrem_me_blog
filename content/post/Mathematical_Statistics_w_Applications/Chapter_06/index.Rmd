---
title: "Chapter 6 - Special Probability Densities"
author: Jeremy Buss
output: html_document
date: "`r Sys.Date()`"
categories:
  - John E. Freund's Mathematical Statistics with Applications - Miller/Miller
  - Statistics
draft: yes
katex: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r message=FALSE, warning=FALSE, include=FALSE}
install.packages("blogdown")
```



## 6.1 Introduction

## 6.2 The Uniform Distribution

**Uniform Distribution** A random variable \(X\) has a **uniform distribution** and it is referred to as a continuous uniform random varaible if and only if its probability density is given by

\[
u(x;\alpha,\beta)= 
\begin{cases}
\frac{1}{\beta - \alpha} & \text{for } \alpha \lt x \lt \beta \\
0 & \text{elsewhere}
\end{cases}
\]

**Theorem 6.1 [Uniform Distribution mean and variance]** The mean and variance of the uniform distribution are given by

\[
\mu = \frac{\alpha + \beta}{2} \;\; \text{and} \;\; \sigma^2=\frac{1}{12}(\beta - \alpha)^2
\]

## 6.3 The Gamma, Exponential, and Chi-Square Distributions

**Gamma Function Integral Definition** 

\[
\Gamma(\alpha) = \int_0^{\infty} y^{\alpha-1} e^{-y} dy \; \; \text{for } \alpha \gt 0
\]

**Gamma Function Factorial Definition** 

\[
\Gamma(\alpha) = (\alpha - 1) \Gamma(\alpha - 1)
\]

or another way

\[
\Gamma(\alpha) = (\alpha - 1)!
\]

**Gamma Distribution** - A random variable \(X\) has a **gamma distribution** and it is referred to as a gamma random variable if and only if its probability density is given by

\[
g(x; \alpha, \beta)
\begin{cases}
\frac{1}{\beta^\alpha \Gamma(\alpha)}x^{\alpha - 1} e^{-x/\beta} & \text{for } x \gt 0\\
0 & \text{elsewhere}
\end{cases}
\]

where \(\alpha > 0\) and \(\beta \gt 0\)

**Exponential Distribution** - A random variable \(X\) has an **exponential distribution** and it is referred to as an exponential random variable if and only if its probability density is given by

\[
g(x; \theta)
\begin{cases}
\frac{1}{\theta} e^{-x/\theta} & \text{for } x \gt 0\\
0 & \text{elsewhere}
\end{cases}
\]

where \(\theta > 0\)

**Chi-Square Distribution** - A random variable \(X\) has a **chi-square distribution** and it is referred to as a chi-square random variable if and only if its probability density is given by

\[
f(x; \nu)
\begin{cases}
\frac{1}{2^{\nu/2} \Gamma(\nu / 2)} x^{\frac{\nu-2}{2}} e^{\frac{-x}{2}} & \text{for } x \gt 0\\
0 & \text{elsewhere}
\end{cases}
\]

where \(\theta > 0\)

**Theorem 6.2 [Gamma Distribution - \(r\)th moment about Origin]**

\[
\mu^{'}_r = \frac{\beta^r\Gamma(\alpha + r)}{\Gamma(\alpha)}
\]

**Theorem 6.2 [Gamma Distribution - \(r\)th moment about Origin - Proof]**

Using definition of moments about the origin

\[
\mu^{'}_r = \int_o^{\infty} x^r \cdot \frac{1}{\beta^{\alpha} \Gamma(\alpha)} x^{\alpha-1} e^{-x/\beta} dx = \frac{\beta^r}{\Gamma(\alpha)} \cdot \int_0^{\infty} y^{\alpha+r-1} e^{-y} dy
\]

where we let \(y=\frac{x}{\beta}\). Since the integral on the right is \(\Gamma(r+\alpha)\) according to the definition of the gamma function.

**Theorem 6.3 [Gamma distribution mean and variance]**

\[
\mu = \alpha \beta \;\; \text{and} \;\; \sigma^2 = \alpha \beta^2
\]

**Theorem 6.3 [Gamma distribution mean and variance - Proof]**

Using Theorem 6.2 [Gamma Distribution - \(r\)th moment about Origin] with \(r=1\) and \(r=2\), we get

\[
\mu_1^{'} = \frac{\beta \Gamma (\alpha + 1)}{\Gamma(\alpha)} = \alpha \beta
\]

and 

\[
\mu_2^{'} = \frac{\beta^2 \Gamma (\alpha + 2)}{\Gamma(\alpha)} = \alpha (\alpha + 1) \beta^2
\]

so \(\mu=\alpha \beta \) and \(\sigma^2=\alpha (\alpha + 1 )\beta^2 - (\alpha \beta)^2=\alpha \beta^2\)

**Exponential Distribution [mean and variance]**

\[
\mu = \theta \;\; \text{and} \;\; \sigma^2=\theta^2
\]

**Chi-square Distribution [mean and variance]**

\[
\mu = \nu \;\; \text{and} \;\; \sigma^2=2\nu
\]

**Theorem 6.4 [Gamma Distribution Moment Generating Function]**

\[
M_X(t)=(1-\beta t)^{-\alpha}
\]

## 6.4 The Beta Distribution

**Beta Distribution** - A random variable \(X\) has an **beta distribution** and it is referred to as a beta random variable if and only if its probability density is given by

\[
f(x; \alpha, \beta)
\begin{cases}
\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \cdot \Gamma(\beta)} x^{\alpha -1} (1-x)^{\beta -1} & \text{for } 0 \lt x \lt 1\\
0 & \text{elsewhere}
\end{cases}
\]

where \(\alpha \gt 0\) and \(\beta \gt 0\)

**Beta Distribution [mean and variance]** 

\[
\mu = \frac{\alpha}{\alpha + \beta} \;\; \text{and} \;\; \sigma^2 = \frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}
\]

**Beta Distribution [mean and variance - Proof]** 

By definition

\[
\begin{align}
&=\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \cdot \Gamma(\beta)} \cdot \int_0^1 x \cdot x^{\alpha - 1} (1-x)^{\beta-1} dx\\
&=\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \cdot \Gamma(\beta)} \cdot \frac{\Gamma(\alpha + 1) \cdot \Gamma(\beta)}{\Gamma(\alpha + \beta + 1)}\\
&=\frac{\alpha}{\alpha + \beta}
\end{align}
\]

where we recognized the integral as \(B(\alpha + 1, \beta)\) and made use of the fact that \(\Gamma(\alpha + 1)=\alpha \cdot \Gamma (\alpha)\) and \(\Gamma(\alpha + \beta + 1)=(\alpha + \beta) \cdot \Gamma (\alpha + \beta)\). Similar steps yield

\[
\mu_2^{'}= \frac{(\alpha + 1)\alpha}{(\alpha + \beta + 1)(\alpha + \beta)}
\]

and if follows that

\[
\begin{align}
\sigma^2&=\frac{(\alpha + 1)\alpha}{(\alpha + \beta + 1)(\alpha + \beta)} - \left ( \frac{\alpha}{\alpha + \beta} \right )^2\\
&=\frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta+1)}
\end{align}
\]

## 6.5 The Normal Distribution

**Normal Distribution** A random variable \(X\) has a **normal distribution** and it is referred to as a normal random variable if and only if its probability density is given by

\[
n(x;\mu,\sigma)=\frac{1}{\sigma \sqrt{2 \pi}}e^{-\frac{1}{2} \left ( \frac{x-\mu}{\sigma} \right )^2} \;\; \text{for} \, -\infty \lt x \lt \infty
\]

where \(\sigma \gt 0 \)

**Theorem 6.6 [Normal Distribution Moment generating function]**

\[
M_X(t)=e^{\mu t + \frac{1}{2}\sigma^2 t^2}
\]

**Theorem 6.6 [Normal Distribution Moment generating function - Proof]**

By definition

\[
\begin{align}
M_X(t)&=\int_{-\infty}^{\infty} e^{xt} \cdot \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2} \left ( \frac{x-\mu}{\sigma} \right ) ^2} \\
&=\frac{1}{\sigma \sqrt{2 \pi}} \cdot \int_{-\infty}^{\infty} e^{-\frac{1}{2 \sigma^2} [-2xt\sigma^2+(x-\mu)^2]} dx 
\end{align}
\]

and if we complete the square, that is use the identity

\[
-2xt\sigma^2 + (x-\mu)^2 = [x-(\mu+t \sigma^2)]^2 - 2\mu t \sigma^2 - t^2 \sigma^4
\]

we get

\[
M_X(t) = e^{\mu t + \frac{1}{2} t^2 \sigma^2} \left \{ \frac{1}{\sigma \sqrt{2 \pi}} \cdot \int_{- \infty}^{\infty} e^{-\frac{1}{2} \left [ \frac{x-(\mu+t\sigma^2)}{\sigma} \right ]^2} dx \right \}
\]

Since the quantity inside the braces is the integral from \(-\infty\) to \(\infty\) of a normal density with the parameters \(\mu + t \sigma^2\) and \(\sigma\), and hence is equal to 1, it follows that 

\[
M_X(t) = e^{\mu t + \frac{1}{2} \sigma^2 t^2}
\]

**Standard Normal Distribution** - The normal distribution with \(\mu=0\) and \(\sigma=1\) is referred to as the **standard normal distribution**

**Theorem 6.7 [Z-score]** If \(X\) has a normal distribution with the mean \(\mu\) and the standard deviation \(\sigma\), then

\[
Z= \frac{X-\mu}{\sigma}
\]

has the standard normal distribution.

**Theorem 6.7 [Z-score - Proof]**

Since the relationship between the values of \(X\) and \(Z\) is linear, \(Z\) must take on a value between \(z_1=\frac{x_1 - \mu}{\sigma}\) and \(z_2=\frac{x_2 - \mu}{\sigma}\) when \(X\) takes on a value between \(x_1\) and \(x_2\). Hence we can write:

\[
\begin{align}
P(x_1 \lt X \lt x_2)&=\frac{1}{\sqrt{2 \pi \sigma}} \int_{x_1}^{x_2} e^{-\frac{1}{2} \left ( \frac{x-\mu}{\sigma} \right )^2} dx\\
&=\frac{1}{\sqrt{2 \pi}} \int_{z_1}^{z_2} e^{-\frac{1}{2} z^2} dz\\
&=\int_{z_1}^{z_2} n(z;0,1) dz\\
&=P(z_1 \lt Z \lt z_2)\\
\end{align}
\]

Where \(Z\) is seen to be a random variable having the standard normal distribution

## 6.6 The Normal Approximation to the Binomial Distribution

**Theorem 6.8 [Normal Approx to Binomial Distribution]** If \(X\) is a random variable having a binomial distribution with the parameters \(n\) and \(\theta\), then the moment generating function of 

\[
Z = \frac{X-n \theta}{\sqrt{n \theta (1-\theta)}}
\]

approaches that of the standard normal distribution when \(n\rightarrow \infty\)

**Theorem 6.8 [Normal Approx to Binomial Distribution - Proof]**

Making use of Theorem 4.10 and 5.4 [Binomial Moment Generation] we can write

\[
M_Z(t)=M_{\frac{X-\mu}{\sigma}}(t)=e^{- \mu t/\sigma} \cdot \left [ 1 + \theta (e^{t/\sigma} - 1)\right ]^n
\]

where \(\mu=n\theta\) and \(\sigma = \sqrt{n \theta (1 - \theta)}\). Then, taking logarithms and substituting the Maclaurin's series of \(e^{t/\sigma}\), we get

\[
\begin{align}
ln M_{\frac{X-\mu}{\sigma}} (t) &= - \frac{\mu t}{\sigma} + n \cdot ln[1+ \theta(e^{t/\sigma }-1)] \\
&=- \frac{\mu t}{\sigma} + n \cdot ln \left [ 1+ \theta \left \{ \frac{t}{\sigma} + \frac{1}{2} \left ( \frac{t}{\sigma} \right )^2 + \frac{1}{6} \left ( \frac{t}{\sigma} \right )^3 + \cdots \right \} \right ] \\
\end{align}
\]

and, using the infinite series \(\ln(1+x)=x - \frac{1}{2} x^2 + \frac{1}{3} x^3 + \cdots\), which converges for \(|x| < 1\), to expand this logarithm, if follows that

\[
\begin{align}
ln M_{\frac{X-\mu}{\sigma}} (t) = - \frac{\mu t}{\sigma} &+ n \theta \left [ \frac{t}{\sigma} + \frac{1}{2} \left ( \frac{t}{\sigma} \right )^2 + \frac{1}{6} \left ( \frac{t}{\sigma} \right )^3 + \cdots \right ] \\
&- \frac{n \theta^2}{2} \left [ \frac{t}{\sigma} + \frac{1}{2} \left ( \frac{t}{\sigma} \right )^2 + \frac{1}{6} \left ( \frac{t}{\sigma} \right )^3 + \cdots \right ]^2\\
&+ \frac{n \theta^3}{3} \left [ \frac{t}{\sigma} + \frac{1}{2} \left ( \frac{t}{\sigma} \right )^2 + \frac{1}{6} \left ( \frac{t}{\sigma} \right )^3 + \cdots \right ]^3\\
&- \cdots\\
\end{align}
\]

Collecting power of \(t\), we obtain

\[
\begin{align}
\ln M_{\frac{X-\mu}{\sigma}} (t) =&  \left ( - \frac{\mu}{\sigma} + \frac{n\theta}{\sigma} \right) t + \left ( \frac{n\theta}{2\sigma^2} - \frac{n\theta^2}{2\sigma^2} \right ) t^2 \\
+&\left ( \frac{n\theta}{6\sigma^3} - \frac{n\theta^2}{2\sigma^3} + \frac{n\theta^3}{3\sigma^3} \right ) t^3 + \cdots \\
&=\frac{1}{\sigma^2} \left ( \frac{n\theta - n\theta^2}{2}\right )t^2 + \frac{n}{\sigma^3} \left ( \frac{\theta - 3\theta^2 +2 \theta^3}{6}\right )t^3\\
\end{align}
\]

since \(\mu = n \theta\). Then, substituting \(\sigma = \sqrt{n \theta (1 - \theta)}\), we find that

\[
\ln M_{\frac{X-\mu}{\sigma}} (t) = \frac{1}{2} t^2 + \frac{n}{\sigma^3} \left ( \frac{\theta - 3 \theta^2 + 2 \theta^3}{6} \right ) t^3 + \cdots
\]

For \(r \gt 2\) the coefficient of \(t^r\) is a constant time \( \frac{n}{\sigma^r}), which approaches 0 when \( n \rightarrow \infty \). It follows that

\[
\lim_{n \rightarrow \infty} \ln M_{\frac{X-\mu}{\sigma}} (t) = \frac{1}{2} t^2
\]

and since the limit of a logarithm equals the logarithm of the limit (provided the two limits exists), we conclude that 

\[
\lim_{n \rightarrow \infty} M_{\frac{X-\mu}{\sigma}} (t) = e^{\frac{1}{2} t^2}
\]

whic is the moment-generating function of Theorem 6.6 [Normal Distribution Moment generating function] with \(\mu=0\) and \( \sigma = 1\)

## 6.7 Bivariate Normal Distribution

**Bivariate Normal Distribution** a pair of random variables \(X\) and \(Y\) have a **bivariate normal distribution** and they are referred to as jointly normally distributed random varaibles if and only if their joint probability density is given by

\[
f(x,y) = \frac{e^{ - \frac{1}{2(1 - \rho)^2} \left [ \left ( \frac{x-\mu_1}{\sigma_1} \right )^2 - 2 \rho \left ( \frac{x-\mu_1}{\sigma_1} \right )\left ( \frac{y -\mu_2}{\sigma_2} \right )+\left ( \frac{y-\mu_2}{\sigma_2} \right )^2 \right ]}}{2 \pi \sigma_1 \sigma_2 \sqrt{1 - \rho^2}}
\]

for \(- \infty \lt x \lt \infty \) and \(- \infty \lt y \lt \infty \), where \(\sigma_1 \gt 0, \sigma_2 \gt 0,\) and \(-1 \lt \rho \lt 1\)

**Theorem 6.9 [Bivariate Normal Distribution Conditional Density]** If \(X\) and \(Y\) have a bivariate normal distribution, the conditional density of \(Y\) and \(X=x\) is a normal distribution with the mean

\[
\mu_{Y|x}=\mu_2 + \rho \frac{\sigma_2}{\sigma_1} (x-\mu_1)
\]

and the variance 

\[
\sigma_{Y|x}^2 = \sigma_2^2(1-\rho^2)
\]

and the conditional density of \(X\) and \(Y=y\) is a normal distribution with the mean 

\[
\mu_{X|y}=\mu_1 + \rho \frac{\sigma_1}{\sigma_2} (y-\mu_2)
\]

and the variance 

\[
\sigma_{X|y}^2 = \sigma_1^2(1-\rho^2)
\]

**Theorem 6.9 [Bivariate Normal Distribution Conditional Density - Proof]** Writing \(w(y|x) = \frac{f(x,y)}{g(x)}\) in accordance with the definition of Conditional Density and letting \(u=\frac{x-\mu_1}{\sigma_1}\) and \(\nu=\frac{y-\mu_2}{\sigma_2}\) to simplify the notation, we get

\[
\begin{align}
w(y|x)&=\frac{\frac{1}{2 \pi \sigma_1 \sigma_2 \sqrt{1-\rho^2}}e^{-\frac{1}{2(1-\rho^2)}[u^2-2 \rho u \nu + \nu^2]}}{\frac{1}{\sqrt{2 \pi \sigma_1}}e^{-\frac{1}{2}u^2}}\\
&=\frac{1}{\sqrt{2 \pi \sigma_2} \sqrt{1-\rho^2}} e^{-\frac{1}{2(1-\rho^2)}[\nu^2-2 \rho u \nu + \rho^2 u^2]}\\
&=\frac{1}{\sqrt{2 \pi \sigma_2} \sqrt{1-\rho^2}} e^{-\frac{1}{2}\left [ \frac{\nu - pu}{\sqrt{1-\rho^2}} \right ] ^2}\\
\end{align}
\]

Then, expressing this result in terms of the original variables, we obtain

\[
w(y|x) = \frac{1}{\sigma_2 \sqrt{2 \pi}\sqrt{1-\rho^2}} e ^{- \frac{1}{2} \left [ \frac{y- \{\mu_2 + \rho \frac{\sigma_2}{\sigma_1}(x-\mu_1)}{\sigma_2 \sqrt{1-\rho^2}}\right ]^2}
\]

for \(- \infty \lt y \lt \infty\), and it can be seen by inspection that this is a normal density with the mean \(\mu_{Y|x} = \mu_2 + \rho \frac{\sigma_2}{\sigma_1}(x - \mu_1)\) and the variance \(\sigma_{Y|x}^2=\sigma_2^2(1 -\rho^2)\). The corresponding results for the conditional density of \(X\) given \(Y=y\) follow by symmetry.

**Theorem 6.10 [Bivariate Normal Distribution Independence]** If two random variables have a bivariate normal distribution, they are independent if and only if \(\rho=0\)