---
title: "Chapter 6 - Special Probability Densities"
author: Jeremy Buss
output: html_document
date: "2023-01-02"
categories:
  - John E. Freund's Mathematical Statistics with Applications - Miller/Miller
  - Statistics
draft: yes
katex: yes
---



<div id="introduction" class="section level2">
<h2>6.1 Introduction</h2>
</div>
<div id="the-uniform-distribution" class="section level2">
<h2>6.2 The Uniform Distribution</h2>
<p><strong>Uniform Distribution</strong> A random variable <span class="math inline">\(X\)</span> has a <strong>uniform distribution</strong> and it is referred to as a continuous uniform random varaible if and only if its probability density is given by</p>
<p><span class="math display">\[
u(x;\alpha,\beta)=
\begin{cases}
\frac{1}{\beta - \alpha} &amp; \text{for } \alpha \lt x \lt \beta \\
0 &amp; \text{elsewhere}
\end{cases}
\]</span></p>
<p><strong>Theorem 6.1 [Uniform Distribution mean and variance]</strong> The mean and variance of the uniform distribution are given by</p>
<p><span class="math display">\[
\mu = \frac{\alpha + \beta}{2} \;\; \text{and} \;\; \sigma^2=\frac{1}{12}(\beta - \alpha)^2
\]</span></p>
</div>
<div id="the-gamma-exponential-and-chi-square-distributions" class="section level2">
<h2>6.3 The Gamma, Exponential, and Chi-Square Distributions</h2>
<p><strong>Gamma Function Integral Definition</strong></p>
<p><span class="math display">\[
\Gamma(\alpha) = \int_0^{\infty} y^{\alpha-1} e^{-y} dy \; \; \text{for } \alpha \gt 0
\]</span></p>
<p><strong>Gamma Function Factorial Definition</strong></p>
<p><span class="math display">\[
\Gamma(\alpha) = (\alpha - 1) \Gamma(\alpha - 1)
\]</span></p>
<p>or another way</p>
<p><span class="math display">\[
\Gamma(\alpha) = (\alpha - 1)!
\]</span></p>
<p><strong>Gamma Distribution</strong> - A random variable <span class="math inline">\(X\)</span> has a <strong>gamma distribution</strong> and it is referred to as a gamma random variable if and only if its probability density is given by</p>
<p><span class="math display">\[
g(x; \alpha, \beta)
\begin{cases}
\frac{1}{\beta^\alpha \Gamma(\alpha)}x^{\alpha - 1} e^{-x/\beta} &amp; \text{for } x \gt 0\\
0 &amp; \text{elsewhere}
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(\alpha &gt; 0\)</span> and <span class="math inline">\(\beta \gt 0\)</span></p>
<p><strong>Exponential Distribution</strong> - A random variable <span class="math inline">\(X\)</span> has an <strong>exponential distribution</strong> and it is referred to as an exponential random variable if and only if its probability density is given by</p>
<p><span class="math display">\[
g(x; \theta)
\begin{cases}
\frac{1}{\theta} e^{-x/\theta} &amp; \text{for } x \gt 0\\
0 &amp; \text{elsewhere}
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(\theta &gt; 0\)</span></p>
<p><strong>Chi-Square Distribution</strong> - A random variable <span class="math inline">\(X\)</span> has a <strong>chi-square distribution</strong> and it is referred to as a chi-square random variable if and only if its probability density is given by</p>
<p><span class="math display">\[
f(x; \nu)
\begin{cases}
\frac{1}{2^{\nu/2} \Gamma(\nu / 2)} x^{\frac{\nu-2}{2}} e^{\frac{-x}{2}} &amp; \text{for } x \gt 0\\
0 &amp; \text{elsewhere}
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(\theta &gt; 0\)</span></p>
<p><strong>Theorem 6.2 [Gamma Distribution - <span class="math inline">\(r\)</span>th moment about Origin]</strong></p>
<p><span class="math display">\[
\mu^{&#39;}_r = \frac{\beta^r\Gamma(\alpha + r)}{\Gamma(\alpha)}
\]</span></p>
<p><strong>Theorem 6.2 [Gamma Distribution - <span class="math inline">\(r\)</span>th moment about Origin - Proof]</strong></p>
<p>Using definition of moments about the origin</p>
<p><span class="math display">\[
\mu^{&#39;}_r = \int_o^{\infty} x^r \cdot \frac{1}{\beta^{\alpha} \Gamma(\alpha)} x^{\alpha-1} e^{-x/\beta} dx = \frac{\beta^r}{\Gamma(\alpha)} \cdot \int_0^{\infty} y^{\alpha+r-1} e^{-y} dy
\]</span></p>
<p>where we let <span class="math inline">\(y=\frac{x}{\beta}\)</span>. Since the integral on the right is <span class="math inline">\(\Gamma(r+\alpha)\)</span> according to the definition of the gamma function.</p>
<p><strong>Theorem 6.3 [Gamma distribution mean and variance]</strong></p>
<p><span class="math display">\[
\mu = \alpha \beta \;\; \text{and} \;\; \sigma^2 = \alpha \beta^2
\]</span></p>
<p><strong>Theorem 6.3 [Gamma distribution mean and variance - Proof]</strong></p>
<p>Using Theorem 6.2 [Gamma Distribution - <span class="math inline">\(r\)</span>th moment about Origin] with <span class="math inline">\(r=1\)</span> and <span class="math inline">\(r=2\)</span>, we get</p>
<p><span class="math display">\[
\mu_1^{&#39;} = \frac{\beta \Gamma (\alpha + 1)}{\Gamma(\alpha)} = \alpha \beta
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mu_2^{&#39;} = \frac{\beta^2 \Gamma (\alpha + 2)}{\Gamma(\alpha)} = \alpha (\alpha + 1) \beta^2
\]</span></p>
<p>so <span class="math inline">\(\mu=\alpha \beta\)</span> and <span class="math inline">\(\sigma^2=\alpha (\alpha + 1 )\beta^2 - (\alpha \beta)^2=\alpha \beta^2\)</span></p>
<p><strong>Exponential Distribution [mean and variance]</strong></p>
<p><span class="math display">\[
\mu = \theta \;\; \text{and} \;\; \sigma^2=\theta^2
\]</span></p>
<p><strong>Chi-square Distribution [mean and variance]</strong></p>
<p><span class="math display">\[
\mu = \nu \;\; \text{and} \;\; \sigma^2=2\nu
\]</span></p>
<p><strong>Theorem 6.4 [Gamma Distribution Moment Generating Function]</strong></p>
<p><span class="math display">\[
M_X(t)=(1-\beta t)^{-\alpha}
\]</span></p>
</div>
<div id="the-beta-distribution" class="section level2">
<h2>6.4 The Beta Distribution</h2>
<p><strong>Beta Distribution</strong> - A random variable <span class="math inline">\(X\)</span> has an <strong>beta distribution</strong> and it is referred to as a beta random variable if and only if its probability density is given by</p>
<p><span class="math display">\[
f(x; \alpha, \beta)
\begin{cases}
\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \cdot \Gamma(\beta)} x^{\alpha -1} (1-x)^{\beta -1} &amp; \text{for } 0 \lt x \lt 1\\
0 &amp; \text{elsewhere}
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(\alpha \gt 0\)</span> and <span class="math inline">\(\beta \gt 0\)</span></p>
<p><strong>Beta Distribution [mean and variance]</strong></p>
<p><span class="math display">\[
\mu = \frac{\alpha}{\alpha + \beta} \;\; \text{and} \;\; \sigma^2 = \frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}
\]</span></p>
<p><strong>Beta Distribution [mean and variance - Proof]</strong></p>
<p>By definition</p>
<p><span class="math display">\[
\begin{align}
&amp;=\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \cdot \Gamma(\beta)} \cdot \int_0^1 x \cdot x^{\alpha - 1} (1-x)^{\beta-1} dx\\
&amp;=\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \cdot \Gamma(\beta)} \cdot \frac{\Gamma(\alpha + 1) \cdot \Gamma(\beta)}{\Gamma(\alpha + \beta + 1)}\\
&amp;=\frac{\alpha}{\alpha + \beta}
\end{align}
\]</span></p>
<p>where we recognized the integral as <span class="math inline">\(B(\alpha + 1, \beta)\)</span> and made use of the fact that <span class="math inline">\(\Gamma(\alpha + 1)=\alpha \cdot \Gamma (\alpha)\)</span> and <span class="math inline">\(\Gamma(\alpha + \beta + 1)=(\alpha + \beta) \cdot \Gamma (\alpha + \beta)\)</span>. Similar steps yield</p>
<p><span class="math display">\[
\mu_2^{&#39;}= \frac{(\alpha + 1)\alpha}{(\alpha + \beta + 1)(\alpha + \beta)}
\]</span></p>
<p>and if follows that</p>
<p><span class="math display">\[
\begin{align}
\sigma^2&amp;=\frac{(\alpha + 1)\alpha}{(\alpha + \beta + 1)(\alpha + \beta)} - \left ( \frac{\alpha}{\alpha + \beta} \right )^2\\
&amp;=\frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta+1)}
\end{align}
\]</span></p>
</div>
<div id="the-normal-distribution" class="section level2">
<h2>6.5 The Normal Distribution</h2>
<p><strong>Normal Distribution</strong> A random variable <span class="math inline">\(X\)</span> has a <strong>normal distribution</strong> and it is referred to as a normal random variable if and only if its probability density is given by</p>
<p><span class="math display">\[
n(x;\mu,\sigma)=\frac{1}{\sigma \sqrt{2 \pi}}e^{-\frac{1}{2} \left ( \frac{x-\mu}{\sigma} \right )^2} \;\; \text{for} \, -\infty \lt x \lt \infty
\]</span></p>
<p>where <span class="math inline">\(\sigma \gt 0\)</span></p>
<p><strong>Theorem 6.6 [Normal Distribution Moment generating function]</strong></p>
<p><span class="math display">\[
M_X(t)=e^{\mu t + \frac{1}{2}\sigma^2 t^2}
\]</span></p>
<p><strong>Theorem 6.6 [Normal Distribution Moment generating function - Proof]</strong></p>
<p>By definition</p>
<p><span class="math display">\[
\begin{align}
M_X(t)&amp;=\int_{-\infty}^{\infty} e^{xt} \cdot \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2} \left ( \frac{x-\mu}{\sigma} \right ) ^2} \\
&amp;=\frac{1}{\sigma \sqrt{2 \pi}} \cdot \int_{-\infty}^{\infty} e^{-\frac{1}{2 \sigma^2} [-2xt\sigma^2+(x-\mu)^2]} dx
\end{align}
\]</span></p>
<p>and if we complete the square, that is use the identity</p>
<p><span class="math display">\[
-2xt\sigma^2 + (x-\mu)^2 = [x-(\mu+t \sigma^2)]^2 - 2\mu t \sigma^2 - t^2 \sigma^4
\]</span></p>
<p>we get</p>
<p><span class="math display">\[
M_X(t) = e^{\mu t + \frac{1}{2} t^2 \sigma^2} \left \{ \frac{1}{\sigma \sqrt{2 \pi}} \cdot \int_{- \infty}^{\infty} e^{-\frac{1}{2} \left [ \frac{x-(\mu+t\sigma^2)}{\sigma} \right ]^2} dx \right \}
\]</span></p>
<p>Since the quantity inside the braces is the integral from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span> of a normal density with the parameters <span class="math inline">\(\mu + t \sigma^2\)</span> and <span class="math inline">\(\sigma\)</span>, and hence is equal to 1, it follows that</p>
<p><span class="math display">\[
M_X(t) = e^{\mu t + \frac{1}{2} \sigma^2 t^2}
\]</span></p>
<p><strong>Standard Normal Distribution</strong> - The normal distribution with <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma=1\)</span> is referred to as the <strong>standard normal distribution</strong></p>
<p><strong>Theorem 6.7 [Z-score]</strong> If <span class="math inline">\(X\)</span> has a normal distribution with the mean <span class="math inline">\(\mu\)</span> and the standard deviation <span class="math inline">\(\sigma\)</span>, then</p>
<p><span class="math display">\[
Z= \frac{X-\mu}{\sigma}
\]</span></p>
<p>has the standard normal distribution.</p>
<p><strong>Theorem 6.7 [Z-score - Proof]</strong></p>
<p>Since the relationship between the values of <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> is linear, <span class="math inline">\(Z\)</span> must take on a value between <span class="math inline">\(z_1=\frac{x_1 - \mu}{\sigma}\)</span> and <span class="math inline">\(z_2=\frac{x_2 - \mu}{\sigma}\)</span> when <span class="math inline">\(X\)</span> takes on a value between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. Hence we can write:</p>
<p><span class="math display">\[
\begin{align}
P(x_1 \lt X \lt x_2)&amp;=\frac{1}{\sqrt{2 \pi \sigma}} \int_{x_1}^{x_2} e^{-\frac{1}{2} \left ( \frac{x-\mu}{\sigma} \right )^2} dx\\
&amp;=\frac{1}{\sqrt{2 \pi}} \int_{z_1}^{z_2} e^{-\frac{1}{2} z^2} dz\\
&amp;=\int_{z_1}^{z_2} n(z;0,1) dz\\
&amp;=P(z_1 \lt Z \lt z_2)\\
\end{align}
\]</span></p>
<p>Where <span class="math inline">\(Z\)</span> is seen to be a random variable having the standard normal distribution</p>
</div>
<div id="the-normal-approximation-to-the-binomial-distribution" class="section level2">
<h2>6.6 The Normal Approximation to the Binomial Distribution</h2>
<p><strong>Theorem 6.8 [Normal Approx to Binomial Distribution]</strong> If <span class="math inline">\(X\)</span> is a random variable having a binomial distribution with the parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(\theta\)</span>, then the moment generating function of</p>
<p><span class="math display">\[
Z = \frac{X-n \theta}{\sqrt{n \theta (1-\theta)}}
\]</span></p>
<p>approaches that of the standard normal distribution when <span class="math inline">\(n\rightarrow \infty\)</span></p>
<p><strong>Theorem 6.8 [Normal Approx to Binomial Distribution - Proof]</strong></p>
<p>Making use of Theorem 4.10 and 5.4 [Binomial Moment Generation] we can write</p>
<p><span class="math display">\[
M_Z(t)=M_{\frac{X-\mu}{\sigma}}(t)=e^{- \mu t/\sigma} \cdot \left [ 1 + \theta (e^{t/\sigma} - 1)\right ]^n
\]</span></p>
<p>where <span class="math inline">\(\mu=n\theta\)</span> and <span class="math inline">\(\sigma = \sqrt{n \theta (1 - \theta)}\)</span>. Then, taking logarithms and substituting the Maclaurinâ€™s series of <span class="math inline">\(e^{t/\sigma}\)</span>, we get</p>
<p><span class="math display">\[
\begin{align}
ln M_{\frac{X-\mu}{\sigma}} (t) &amp;= - \frac{\mu t}{\sigma} + n \cdot ln[1+ \theta(e^{t/\sigma }-1)] \\
&amp;=- \frac{\mu t}{\sigma} + n \cdot ln \left [ 1+ \theta \left \{ \frac{t}{\sigma} + \frac{1}{2} \left ( \frac{t}{\sigma} \right )^2 + \frac{1}{6} \left ( \frac{t}{\sigma} \right )^3 + \cdots \right \} \right ] \\
\end{align}
\]</span></p>
<p>and, using the infinite series <span class="math inline">\(\ln(1+x)=x - \frac{1}{2} x^2 + \frac{1}{3} x^3 + \cdots\)</span>, which converges for <span class="math inline">\(|x| &lt; 1\)</span>, to expand this logarithm, if follows that</p>
<p><span class="math display">\[
\begin{align}
ln M_{\frac{X-\mu}{\sigma}} (t) = - \frac{\mu t}{\sigma} &amp;+ n \theta \left [ \frac{t}{\sigma} + \frac{1}{2} \left ( \frac{t}{\sigma} \right )^2 + \frac{1}{6} \left ( \frac{t}{\sigma} \right )^3 + \cdots \right ] \\
&amp;- \frac{n \theta^2}{2} \left [ \frac{t}{\sigma} + \frac{1}{2} \left ( \frac{t}{\sigma} \right )^2 + \frac{1}{6} \left ( \frac{t}{\sigma} \right )^3 + \cdots \right ]^2\\
&amp;+ \frac{n \theta^3}{3} \left [ \frac{t}{\sigma} + \frac{1}{2} \left ( \frac{t}{\sigma} \right )^2 + \frac{1}{6} \left ( \frac{t}{\sigma} \right )^3 + \cdots \right ]^3\\
&amp;- \cdots\\
\end{align}
\]</span></p>
<p>Collecting power of <span class="math inline">\(t\)</span>, we obtain</p>
<p><span class="math display">\[
\begin{align}
\ln M_{\frac{X-\mu}{\sigma}} (t) =&amp;  \left ( - \frac{\mu}{\sigma} + \frac{n\theta}{\sigma} \right) t + \left ( \frac{n\theta}{2\sigma^2} - \frac{n\theta^2}{2\sigma^2} \right ) t^2 \\
+&amp;\left ( \frac{n\theta}{6\sigma^3} - \frac{n\theta^2}{2\sigma^3} + \frac{n\theta^3}{3\sigma^3} \right ) t^3 + \cdots \\
&amp;=\frac{1}{\sigma^2} \left ( \frac{n\theta - n\theta^2}{2}\right )t^2 + \frac{n}{\sigma^3} \left ( \frac{\theta - 3\theta^2 +2 \theta^3}{6}\right )t^3\\
\end{align}
\]</span></p>
<p>since <span class="math inline">\(\mu = n \theta\)</span>. Then, substituting <span class="math inline">\(\sigma = \sqrt{n \theta (1 - \theta)}\)</span>, we find that</p>
<p><span class="math display">\[
\ln M_{\frac{X-\mu}{\sigma}} (t) = \frac{1}{2} t^2 + \frac{n}{\sigma^3} \left ( \frac{\theta - 3 \theta^2 + 2 \theta^3}{6} \right ) t^3 + \cdots
\]</span></p>
<p>For <span class="math inline">\(r \gt 2\)</span> the coefficient of <span class="math inline">\(t^r\)</span> is a constant time <span class="math inline">\(\frac{n}{\sigma^r}), which approaches 0 when \( n \rightarrow \infty\)</span>. It follows that</p>
<p><span class="math display">\[
\lim_{n \rightarrow \infty} \ln M_{\frac{X-\mu}{\sigma}} (t) = \frac{1}{2} t^2
\]</span></p>
<p>and since the limit of a logarithm equals the logarithm of the limit (provided the two limits exists), we conclude that</p>
<p><span class="math display">\[
\lim_{n \rightarrow \infty} M_{\frac{X-\mu}{\sigma}} (t) = e^{\frac{1}{2} t^2}
\]</span></p>
<p>whic is the moment-generating function of Theorem 6.6 [Normal Distribution Moment generating function] with <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma = 1\)</span></p>
</div>
<div id="bivariate-normal-distribution" class="section level2">
<h2>6.7 Bivariate Normal Distribution</h2>
<p><strong>Bivariate Normal Distribution</strong> a pair of random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have a <strong>bivariate normal distribution</strong> and they are referred to as jointly normally distributed random varaibles if and only if their joint probability density is given by</p>
<p><span class="math display">\[
f(x,y) = \frac{e^{ - \frac{1}{2(1 - \rho)^2} \left [ \left ( \frac{x-\mu_1}{\sigma_1} \right )^2 - 2 \rho \left ( \frac{x-\mu_1}{\sigma_1} \right )\left ( \frac{y -\mu_2}{\sigma_2} \right )+\left ( \frac{y-\mu_2}{\sigma_2} \right )^2 \right ]}}{2 \pi \sigma_1 \sigma_2 \sqrt{1 - \rho^2}}
\]</span></p>
<p>for <span class="math inline">\(- \infty \lt x \lt \infty\)</span> and <span class="math inline">\(- \infty \lt y \lt \infty\)</span>, where <span class="math inline">\(\sigma_1 \gt 0, \sigma_2 \gt 0,\)</span> and <span class="math inline">\(-1 \lt \rho \lt 1\)</span></p>
<p><strong>Theorem 6.9 [Bivariate Normal Distribution Conditional Density]</strong> If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have a bivariate normal distribution, the conditional density of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X=x\)</span> is a normal distribution with the mean</p>
<p><span class="math display">\[
\mu_{Y|x}=\mu_2 + \rho \frac{\sigma_2}{\sigma_1} (x-\mu_1)
\]</span></p>
<p>and the variance</p>
<p><span class="math display">\[
\sigma_{Y|x}^2 = \sigma_2^2(1-\rho^2)
\]</span></p>
<p>and the conditional density of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y=y\)</span> is a normal distribution with the mean</p>
<p><span class="math display">\[
\mu_{X|y}=\mu_1 + \rho \frac{\sigma_1}{\sigma_2} (y-\mu_2)
\]</span></p>
<p>and the variance</p>
<p><span class="math display">\[
\sigma_{X|y}^2 = \sigma_1^2(1-\rho^2)
\]</span></p>
<p><strong>Theorem 6.9 [Bivariate Normal Distribution Conditional Density - Proof]</strong> Writing <span class="math inline">\(w(y|x) = \frac{f(x,y)}{g(x)}\)</span> in accordance with the definition of Conditional Density and letting <span class="math inline">\(u=\frac{x-\mu_1}{\sigma_1}\)</span> and <span class="math inline">\(\nu=\frac{y-\mu_2}{\sigma_2}\)</span> to simplify the notation, we get</p>
<p><span class="math display">\[
\begin{align}
w(y|x)&amp;=\frac{\frac{1}{2 \pi \sigma_1 \sigma_2 \sqrt{1-\rho^2}}e^{-\frac{1}{2(1-\rho^2)}[u^2-2 \rho u \nu + \nu^2]}}{\frac{1}{\sqrt{2 \pi \sigma_1}}e^{-\frac{1}{2}u^2}}\\
&amp;=\frac{1}{\sqrt{2 \pi \sigma_2} \sqrt{1-\rho^2}} e^{-\frac{1}{2(1-\rho^2)}[\nu^2-2 \rho u \nu + \rho^2 u^2]}\\
&amp;=\frac{1}{\sqrt{2 \pi \sigma_2} \sqrt{1-\rho^2}} e^{-\frac{1}{2}\left [ \frac{\nu - pu}{\sqrt{1-\rho^2}} \right ] ^2}\\
\end{align}
\]</span></p>
<p>Then, expressing this result in terms of the original variables, we obtain</p>
<p><span class="math display">\[
w(y|x) = \frac{1}{\sigma_2 \sqrt{2 \pi}\sqrt{1-\rho^2}} e ^{- \frac{1}{2} \left [ \frac{y- \{\mu_2 + \rho \frac{\sigma_2}{\sigma_1}(x-\mu_1)}{\sigma_2 \sqrt{1-\rho^2}}\right ]^2}
\]</span></p>
<p>for <span class="math inline">\(- \infty \lt y \lt \infty\)</span>, and it can be seen by inspection that this is a normal density with the mean <span class="math inline">\(\mu_{Y|x} = \mu_2 + \rho \frac{\sigma_2}{\sigma_1}(x - \mu_1)\)</span> and the variance <span class="math inline">\(\sigma_{Y|x}^2=\sigma_2^2(1 -\rho^2)\)</span>. The corresponding results for the conditional density of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y=y\)</span> follow by symmetry.</p>
<p><strong>Theorem 6.10 [Bivariate Normal Distribution Independence]</strong> If two random variables have a bivariate normal distribution, they are independent if and only if <span class="math inline">\(\rho=0\)</span></p>
</div>
