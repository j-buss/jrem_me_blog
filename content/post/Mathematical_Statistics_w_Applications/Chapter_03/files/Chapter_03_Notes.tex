\documentclass[
10pt,reqno
]{amsart}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[rightcaption]{sidecap}
\usepackage{amsthm}
\graphicspath{ {../images/} }
\setcounter{section}{3}
\newtheorem{thm}{Theorem}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\title{Chapter 3 - Probability Distributions and Probability Densities}
\setlength{\parindent}{0pt}
\begin{document}
\maketitle


\section*{3.1 Random Variables}

\begin{definition}[Random Variable]
If \(S\) is a sample space with a probability measure and \(X\) is a real-valued function defined over the elements of \(S\), then \(X\) is called a random variable.
\end{definition}

\textbf{Discrete Random Variables} - random variables whose range is finite or countably infinite

\section*{3.2 Probability Distributions}

\begin{definition}[Probability Distribution]
If \(X\) is a discrete random variable, the function given by \(f(x)=P(X=x)\) for each \(x\) within the range of \(X\) is called the probability distribution of \(X\).
\end{definition}

\begin{thm}[Probability Distribution - Discrete Conditions]
\label{thm:ProbDistDiscrete}
A function can serve as the probability distribution of a discrete random variable \(X\) if and only if its values, \(f(x)\), satisfy the conditions:
\begin{enumerate}
	\item \(f(x) \ge 0\) for each value within its domain
	\item \(\sum_{x} f(x)=1\), where the summation extends over all the values within its domain
\end{enumerate}
\end{thm}

\begin{definition}[Distribution Function]
If \(X\) is a discrete random variable, the function given by
\begin{align*}
F(x)=P(X \leq x) = \sum_{t \leq X} f(t) && \text{for} \,-\infty < x < \infty
\end{align*}
where \(f(t)\) is the value of the probability distribution of \(X\) at \(t\), is called the \textbf{distribution function}, or the \textbf{cumulative distribution} of \(X\)
\end{definition}

\begin{thm}[Distribution Function - Discrete Conditions]
The values \(F(x)\) of the distribution function of a discrete random variable \(X\) satisfy the conditions:
\begin{enumerate}
	\item \(F(- \infty ) = 0\) and \(F(\infty) = 1\)
	\item if \(a < b\), then \(F(a) \leq F(b)\) for any real numbers \(a\) and \(b\)
\end{enumerate}
\end{thm}

\begin{proof}
TBD: Exercise 3.8
\end{proof}

\begin{thm}[Probability Distribution from Cumulative Distribution - Discrete]
If the range of a random variable \(X\) consists of the values \(x_1 < x_2 < x_3 < \cdots < x_n\), then \(f(x_1)=F(x_1)\) and
\begin{align*}
f(x_i)=F(x_i)-F(x_{i-1}) && \text{for} \; i = 2,3,\ldots,n
\end{align*}
\end{thm}


\section*{3.3 Continuous Random Variables}
\textit{This section intentially left blank}

\section*{3.4 Probability Density Functions}

\begin{definition}[Probability Density Function]
A function with values \(f(x)\), defined over the set of all real numbers, is called a \textbf{probability density function} or abreviated as \textbf{pdf} of the continuous random variable \(X\) if and only if 
\[
P(a \leqq X \leq b) = \int_a^b f(x)dx
\]
for any real constants \(a\) and \(b\) with \(a \leq b\)
\end{definition}

\begin{thm}[Endpoints not needed]
If \(X\) is a continuous random variable and \(a\) and \(b\) are real constants with \(a \leq b\), then
\[
P(a \leq X \leq b) = P(a \leq X < b) = P(a < X \leq b) = P(a < X < b)
\]
\end{thm}

\begin{thm}[Probability Density - Continuous Conditions]
A function can serve as a probability density of a continuous random variable \(X\) if its values, \(f(x)\), satisfy the conditions:
\begin{enumerate}
	\item \(f(x) \ge 0\) for \(- \infty < x < \infty \)
	\item \(\int_{-\infty}^{\infty} f(x) dx = 1\)
\end{enumerate}
\end{thm}

\begin{definition}[Distribution Function]
If \(X\) is a continuous random variable and the value of its probability density at \(t\) is \(f(t)\), then the function given by
\begin{align*}
F(x)=P(X \leq x)= \int_{- \infty}^{x} f(t) dt && \text{for} \, -\infty < x < \infty
\end{align*}
is called the \textbf{distribution function} or the \textbf{cumulative distribution function} of \(X\)
\end{definition}

\begin{thm}[Derivative of Distribution Function is Probability Density]
If \(f(x)\) and \(F(x)\) are the values of the probability density and the distribution function of \(X\) at \(x\), then
\[
P(a \leq X \leq b) = F(b)-F(a)
\]
for any real constants \(a\) and \(b\) with \(a \leq b\), and
\[
f(x)=\frac{dF(x)}{dx}
\]
where the derivative exists.
\end{thm}

\section*{3.5 Multivariate Distributions}

\textbf{bivariate} - situations where we are interested at the same time in a pair of random variables defined over a joint sample space
\textbf{multivariate} - situations covering any finite number of random variables
\textbf{univariate} - situations with one random variable

\begin{definition}[Joint Probability Distribution]
If \(X\) and \(Y\) are discrete random variables, the function given by \(f(x,y)=P(X=x,Y=y)\) for each pair of values \((x,y)\) within the range of \(X\) and \(Y\) is called the \textbf{joint probability distribution} of \(X\) and \(Y\).
\end{definition}

\newpage

\begin{thm}[Joint Probability Distribution - Discrete Conditions]
A bivariate function can serve as the joint probability distribution of a pair of discrete random variables \(X\) and \(Y\) if and only if its values, \(f(x,y)\), satisfy the conditions:
\begin{enumerate}
	\item \(f(x,y) \ge 0\) for each pair of values \((x,y)\) within its domain
	\item \(\sum_x \sum_y f(x,y)=1\), where the double summation extends over all possible pairs \((x,y)\) within its domain
\end{enumerate}
\end{thm}

\begin{definition}[Joint Distribution Function]
If \(X\) and \(Y\) are discrete random variables, the function given by
\begin{align*}
F(x,y)=P(X \leq x, Y \leq y) = \sum_{s \leq x} \sum_{t \leq y} f(s,t) && \text{for} \, - \infty < x < \infty\\
&& \text{for} \, - \infty < y < \infty
\end{align*}
where \(f(s,t)\) is the value of the joint probability distribution of \(X\) and \(Y\) at \((s,t)\), is called the \textbf{joint distribution function} or the \textbf{joint cumulative distribution} of \(X\) and \(Y\)
\end{definition}

\begin{definition}[Joint Probability Density Function]
A bivariate function with values \(f(x,y)\) defined over the \(xy\)-plane is called a \textbf{joint probability density function} of the continuous random variables \(X\) \(Y\) if and only if
\[
P(X,Y) \in A = \underset{A}\iint f(x,y)dxdy
\]
for any region \(A\) in the \(xy\)-plane
\end{definition}

\begin{thm}[Joint Probability Density Function - Continuous Conditions]
A bivariate function can serve as a joint probability density function of a pair of continuous random variables \(X\) and \(Y\) if its values, \(f(x,y)\), satisfy the conditions:
\begin{enumerate}
	\item \(f(x,y) \geq 0\) for \(- \infty < x < \infty \), \(- \infty < y < \infty \)
	\item \(\int^\infty_{-\infty} \int^\infty_{-\infty} f(x,y) dx dy = 1\)
\end{enumerate}
\end{thm}

\begin{definition}[Joint Distribution Function]
If \(X\) and \(Y\) are continuous random variables, the function given by 
\begin{align*}
F(x,y)=P(X \leq x, Y \leq y)= \int^\infty_{-\infty} \int^\infty_{-\infty} f(s,t) ds dt && \text{for} -\infty < x < \infty\\
&&-\infty < y < \infty\\
\end{align*}
where \(f(s,t)\) is the joint probability density of \(X\) and \(Y\) at \((s,t)\), is called the \textbf{joint distribution function of X and Y}
\end{definition}

\newpage

\section*{3.6 Marginal Distributions}

\begin{definition}[Marginal Distribution]
If \(X\) and \(Y\) are discrete random variables and \(f(x,y)\) is the value of their joint probability distribution at \((x,y)\), the function given by
\[
g(x)=\underset{y}\sum f(x,y)
\]
for each \(x\) within the range of \(X\) is called the \textbf{marginal distribution of \(X\)}. Correspondingly, the function given by
\[
h(y)=\underset{x}\sum f(x,y)
\]
for each \(y\) within the range of \(Y\) is called the \textbf{marginal distribution of \(Y\)}
\end{definition}

\begin{definition}[Marginal Density]
If \(X\) and \(Y\) are continuous random variables and \(f(x,y)\) is the value of their joint probability density at \((x,y)\), the function given by
\begin{align*}
g(x)= \int_{-\infty}^{\infty} f(x,y)dy && \text{for} -\infty < x < \infty\\
\end{align*}
is called the \textbf{marginal density of \(X\)}. Correspondingly, the function given by 
\begin{align*}
h(x)= \int_{-\infty}^{\infty} f(x,y)dx && \text{for} -\infty < y < \infty\\
\end{align*}
is called the \textbf{marginal density of \(Y\)}.
\end{definition}

\section*{3.7 Conditional Distributions}

\begin{definition}[Conditional Distribution]
If \(f(x,y)\) is the value of the joint probability distribution of the discrete random variables \(X\) and \(Y\) at \((x,y)\) and \(h(y)\) is the value of the marginal distribution of \(Y\) at \(y\), the function given by
\begin{align*}
f(x|y)=\frac{f(x,y)}{h(y)} && h(y) \ne 0
\end{align*}
for each \(x\) within the range of \(X\) is called the \textbf{conditional distribution of \(X\) given \(Y=y\)}. Correspondingly, if \(g(x)\) is the value of the marginal distribution of \(X\) at \(x\), the function given by
\begin{align*}
w(x|y)=\frac{f(x,y)}{g(y)} && g(y) \ne 0
\end{align*}
for each \(y\) within the range of \(Y\) is called the \textbf{conditional distribution of \(Y\) given \(X=x\)}.
\end{definition}

\newpage

\begin{definition}[Conditional Density]
If \(f(x,y)\) is the value of the joint density of the continuous random variables \(X\) and \(Y\) at \((x,y)\) and \(h(y)\) is the value of the marginal distribution of \(Y\) at \(y\), the function given by
\begin{align*}
f(x|y)=\frac{f(x,y)}{h(y)} && h(y) \ne 0
\end{align*}
for \(-\infty < x < \infty \), is called the \textbf{conditional density of \(X\) given \(Y=y\)}. Correspondingly, if \(g(x)\) is the value of the marginal density of \(X\) at \(x\), the function given by
\begin{align*}
w(x|y)=\frac{f(x,y)}{g(y)} && g(y) \ne 0
\end{align*}
for \(-\infty < y < \infty \), is called the \textbf{conditional density of \(Y\) given \(X=x\)}.
\end{definition}

\begin{definition}[Independence of Discrete Random Variables]
If \(f(x_1,x_2,\ldots , x_n)\) is the value of the joint probability distribution of the discrete random variables \(X_1, X_2, \ldots, X_n\) at \((x_1, x_2, \ldots x_n)\) and \(f_i(x_i)\) is the value of the marginal distribution of \(X_i\) at \(x_i\) for \(i=1,2,\ldots,n\), then \(n\) random variables are \textbf{independent} if and only if
\[
f(x_1,x_2,\ldots,x_n)=f_1(x_1)\cdot f_2(x_2)\cdot \ldots f_n(x_n)
\]
for all \((x_1, x_2, \ldots, x_n)\) within their range
\end{definition}

\section*{3.8 Theory in Practice}

\textbf{Frequency Distribution} - A grouping of numerical data into classes having definite upper and lower limits

\textbf{stem-and-leaf display} - device for presenting quantitative data similar to histogram; generally grouped in 10s

\textbf{positive skewness} - long right-hand tail

\textbf{negative skewness} - long left-hand tail

\textbf{mode} - the value that appears most frequently in a data set; in a histogram it may be less general and refer to data values that are high points where the mode is a bar in a histogram that is surrounded by bars of lower frequency

\textbf{bimodal} - histogram exhibiting two modes

\textbf{multimodal} - histogram exhibiting more than two modes


\end{document}