<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on jrem.me - wonder.explore.share</title>
    <link>/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on jrem.me - wonder.explore.share</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Jan 2022 00:00:00 +0000</lastBuildDate><atom:link href="/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Linear Regression</title>
      <link>/2022/01/03/linear-regression/</link>
      <pubDate>Mon, 03 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/2022/01/03/linear-regression/</guid>
      <description>Interaction: two or more variables interact to affect a third variable in a non additive manner, i.e. two variables interact to have an affect more than the sum of their parts Simple Linear Regression (formula): \(Y = \beta_0 + \beta_1 X + \epsilon\) 2 Coefficients or parameters for Simple Linear Regression (SLR): \(\beta_0=\) Intercept \(\beta_1=\) slope SLR estimate (formula): \(\hat{y_i} = \hat{\beta}_0 + \hat{\beta}_1 x_i\) Residual: difference between the \(i\)th observed response and the \(i\)th response value predicted by our model Residual (formula): \(e_i = y_i - \hat{y}_i\) Residual Sum of Squares: the sum of squared residuals for all observations \(i=1, 2, \dots, n\) a.</description>
    </item>
    
    <item>
      <title>Introduction and Linear Algebra Review</title>
      <link>/2022/01/01/introduction-and-linear-algebra-review/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/2022/01/01/introduction-and-linear-algebra-review/</guid>
      <description>Notation and Simple Matrix Algebra n: number of observations
\(x_{ij}\): the \(j\)-th variable of the \(i\)th observation where \(i=1,2,\dots,n\) and \(j=1,2,\dots,p\)
p: number of variables
\(\mathbf{X}\): is a matrix of size \(n \times p\) whose \((i,j)\)th element is \(x_{ij}\)
\(\mathbf{X}=\) \(\begin{pmatrix} x_{11}&amp;amp;x_{12}&amp;amp;\dots&amp;amp;x_{1p} \\x_{21}&amp;amp;x_{22}&amp;amp;\dots&amp;amp;x_{2p}\\\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots\\x_{n1}&amp;amp;x_{n2}&amp;amp;\dots&amp;amp;x_{np} \end{pmatrix}\)
Row vector of \(\mathbf{X}\): \(x_i = \begin{pmatrix} x_{i1}\\x_{i2}\\\vdots\\x_{ip} \end{pmatrix}\);
*Note: even though it is a row vector the default orientation is as a column Column vector of \(\mathbf{X}\): \(\mathbf{x}_j = \begin{pmatrix} x_{1j}\\x_{2j}\\\vdots\\x_{nj} \end{pmatrix}\)</description>
    </item>
    
    <item>
      <title>Statistical Learning</title>
      <link>/2022/01/01/statistical-learning/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/2022/01/01/statistical-learning/</guid>
      <description>Input Variable (X): predictor, independent variable, feature Output Variable(Y): response, dependent variable error term (\(\epsilon\)): assumed independent of \(X, Y\) w/ mean of zero General form of relationship between \(X\)s and \(Y\): \(Y=f(X) + \epsilon\) Estimated relationship: We don’t know \(f(X)\) so we estimate it by using the data to fit a relationship \(\hat{Y}=\hat{f}(X)\) Two reasons to estimate an unknown function \(\hat{f}\) relating input vs. output: Prediction and Inference Error is composed of 2 categories: Reducible and Irreducible Reducible Error: Potentially improve accuracy of \(\hat{f}\) by using the most appropriate statistical learning techniques Irreducible Error: may contain unmeasured items that can’t be used to understand or predict \(Y\) 2 Traits of Parametric Model: 1 - Makes an assumption about the form of the model 2 - Use data to estimate relatively few parameters of the assumed form Non-Parametric Model - No assumptions about the form of the model are made Overfitting: Model follows errors or noise too closely Advantage of Parametric Model: Assumed model requires few estimated parameters Disadvantage of Parametric Model: Assumed model form may not follow true form Advantage of Non-Parametric Model: No assumed form Disadvantage of Non-Parametric Model: Lots of data required to fit model Why would we ever choose to uyse a more restrictive model instead of a very flexible approach?</description>
    </item>
    
  </channel>
</rss>
